---
title: "Math 23c - Week 4"
author: "Paul Bamberg"
output:
  slidy_presentation:
  font_adjustment: +1
incremental: true
widescreen: true
classoption: aspectratio=169
runtime: shiny
---
<style type="text/css">
div.slide p {
  color: DarkBlue;
  font-size:smaller;
}
div.slide h1 {
  color: DarkRed;
  font-size: 100%;
}
div.slide h2 {
  color: DarkGreen;
  font-size: smaller
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```
  
# Partitions

* Our task is to define the integral of a **bounded** function $f$ over a **bounded** interval $[a,b].$  

* On any subinterval $S \subseteq [a,b]$, we write $\sup\{f(x):x\in S\}$ as $M(f, S)$.    
Similarly, we write $\inf\{f(x):x \in S\}$ as $m(f, S)$.  
We use "supremum" instead of "maximum" so that we can use intervals that are not closed, ones on which a bounded function has a supremum but does not necessarily have a maximum.


* A **partition** of the interval $[a,b]$ is an ordered subset $P$ with the following property: 
  $P = \{a = t_0 < t_1< \cdots < t_n =b\}.$  
  While it is convenient to think of the partition as a set of intervals, $P$ refers explicitly to the set of division points.    
  The division points do not have to be equally spaced, and the subintervals need not all have the same length.  
  The **mesh** of the partition is the length of the longest subinterval.
  
# A finer partition can only increase the minimum, decrease the maximum
  * If $P \subseteq Q$, then every division point in $P$ is also a division point in $Q$.   So partition $Q$ (with the extra red points) divides up the subintervals in partition $P$.
  
```{r, echo = FALSE, fig.width = 8, fig.height = 1}
par(mar= c(1,1,1,1))
P <- runif(8, min = 0, max = 1)    
plot(c(P,0,1),rep(0,10))
extra <- runif(4)
points(c(extra),rep(0,4),col = "red")
```
  
  * If $S$ is a subinterval of partition $P$ that gets divided into one or more subintervals, $T_1, T_2, \cdots, T_k$ of $Q$, then $$m(f, S) \leq \min_i m(f, T_i)$$
  
  * Proof: for the $i$th subinterval, the minimum for the subinterval cannot be less than the overall minimum.
  $$\forall i, m(f,S ) \leq m(f,T_i )$$
  $$\text{So  } m(f,S ) \leq \min_i m(f,T_i )$$
  
  *  A similar proof shows that   $$M(f, S) \geq \max_i M(f, T_i)$$
 
# Darboux sums   
    
  * The **upper Darboux sum**, $U(f, P)$ of $f$ with respect to $P$ is  
  $$U(f, P) = \sum_{k=1}^nM(f, [t_{k-1}, t_k])\cdot (t_k - t_{k-1})$$
  
  * The **lower Darboux sum**, $L(f, P)$ of $f$ with respect to $P$ is  
  $$L(f, P) = \sum_{k=1}^nm(f, [t_{k-1}, t_k]) \cdot(t_k - t_{k-1})$$ 
  
  * If $f$ is a bounded function on $[a,b]$, $P$ and $Q$ are partitions of $[a,b]$, and $P \subseteq Q$, then $L(f,P) \leq L(f,Q)$.
  
  * Proof: Each subdivision point in $P$ is also a subdivision point in $Q$, but $Q$ contains subdivision points that are not in $P$. Thus each subinterval in $P$ may be subdivided further in subdivision $Q$. This can only have the effect of increasing the lower Darboux sum, so $L(f,P) \leq L(f,Q)$
  
  * A similar proof shows that $U(f,P) \geq U(f,Q)$.
  
  * Since an infimum can never be larger than a supremum,  $m(f, [a,b])\cdot (b-a) \leq L(f, P) \leq U(f,P) \leq M(f, [a,b])\cdot (b-a)$
    
# Darboux integrals

  * Let $P$ and $Q$ be any two partitions of $[a,b].$  Then $P \cup Q$ is a partition that includes the division points of both $P$(blue) and $Q$(red). 
  ```{r, echo = FALSE, fig.width = 8, fig.height = 1}
par(mar= c(1,1,1,1))
P <- runif(6, min = 0, max = 1)    
plot(c(P,0,1),rep(0,8),col = "blue")
extra <- runif(4)
points(c(extra),rep(0,4),col = "red")
```
 * Now the lower and upper Darboux sums satisfy $L(f,P) \leq U(f,Q)$.
   
 * Proof: $P \subseteq P \cup Q$ so $L(f,P) \leq L(f,P \cup Q)$  
 $Q \subseteq P \cup Q$   so $U(f,Q) \geq U(f,P \cup Q)$  
 $L(f,P) \leq L(f,P \cup Q) \leq U(f,P \cup Q) \leq U(f,Q)$
 
  * Thus $U(f,Q)$ is an upper bound for the set of all lower Darboux sums. By the completeness property of the real numbers, this set of lower Darboux sums has a **least upper bound**.
  
  * The **lower Darboux integral**, $L(f)$, is the **least upper bound** of all possible lower Darboux sums.  
  $L(f) = \sup \{L(f, P): P \text{ is a partition of } [a,b]\}$
  
# The upper Darboux integral
    
  * Similarly, $L(f,P)$ is a lower bound for the set of all upper Darboux sums. By the completeness property of the real numbers, this set of upper Darboux sums has a **greatest lower bound**.   
  
  * The **upper Darboux integral**, $U(f)$, is the **greatest lower bound** of all possible upper Darboux sums.  
  $U(f) = \inf \{U(f, P): P \text{ is a partition of } [a,b]\}$
  
# Integrable and non-integrable functions
  
* On an interval $[a,b]$, any bounded function has a lower Darboux integral $L(f)$ and an upper Darboux integral $U(f)$.
    
* Proof that $L(f)\leq U(f)$.  

  + $\forall Q, L(f,P) \leq U(f,Q)$  
  + $\text{so } L(f,P) \leq \inf U(f,Q)$ 
  + $\text{and } L(f,P) \leq U(f)$.
  + This in turn is true for all $P$, so $\sup L(f,P) = L(f) \leq U(f).$
    
 
* We say $f$ is **integrable** on $[a,b]$ if $L(f) = U(f)$.

* Not every bounded function is integrable.
  For example, on [0,1],  define $f(x) = 1$ for rational $x$, $f(x) = 0$ for irrational $x$.
  
* Then for any partition $P$, $L(f,P) = 0$ (there are irrational points in [0,1])  
while $U(f,P) = 1$ (there are rational points in [0,1]).  
Therefore $L(f) < U(f)$, and $f$ is not integrable on [0,1].

  
  
# A necessary condition for integrability
  
  * A bounded function $f$ on the interval $[a, b]$ is integrable only if $\forall \epsilon >0$, $\exists$ a partition $P$ of $[a,b]$ such that 
  $U(f, P) - L(f, P)<\epsilon$
  
  * Proof: Given a specific $\epsilon > 0$, here is a way to construct $P$ for which   $U(f, P) - L(f, P)<\epsilon$.
  
    + Since $L(f)$ is the least upper bound of $L(f,P)$, we can choose partition $P_1$ for which $$L(f,P_1) > L(f) - \epsilon /2.$$
    + Since $U(f)$ is the greatest lower bound of $U(f,P)$, we can choose partition $P_2$ for which $ $U(f,P_2) > U(f) + \epsilon/2.$$
    + Now set $P = P_1 \cup P_2$. The effect of using this finer partition can only be to increase $L$ and decrease $U$.  
    + So $U(f,P) \leq U(f,P_2) < U(f) + \epsilon/2.$.  
    and $L(f,P) \geq L(f,P_2) < L(f) - \epsilon/2.$.
    + Subtract: $U(f,P) - L(f,P) < U(f) - L(f) + \epsilon$
    + If $f$ is integrable, then $U(f) - L(f) = 0$ and the partition $P$ makes $U(f, P) - L(f, P)<\epsilon$.
    
# A sufficient condition for integrability
  
  * If $\forall \epsilon >0$, $\exists$ a partition $P$ of $[a,b]$ such that 
  $U(f, P) - L(f, P)<\epsilon$, then $f$ is integrable.
  
  * Proof: Given $\epsilon,$ choose $P$ for which $U(f,P) - L(f,P) < \epsilon.$  
  
  + $U(f)$ is the greatest lower bound, so $U(f) \leq U(f,P).$   
  
  + $U(f) < U(f,P)-L(f,P)+ L(f,P) < \epsilon + L(f,P)$  
  
  + Since $L(f)$ is the least upper bound of $L(f,P)$, $U(f) <   \epsilon + L(f)$ and $U(f) - L(f) < \epsilon$.
  
  + This can be true for all $\epsilon >0$ only if $U(f) - L(f) = 0$, which means that $f$ is integrable.
   
# Any increasing function on $[a,b]$ is integrable
  
  * Proof: Given $\epsilon > 0,$ choose a partition $P$ for which the "mesh" (longest subinterval) is  $< \frac{\epsilon}{f(b)-f(a)}$.
  The endpoints of the subintervals are $a < t_1 < t_2 < \cdots < t_n < b$
  
  + On subinterval $[t_{k-1},t_k]$ the maximum is at the right endpoint $t_k$, while the minimum is at the left endpoint $t_k$. 
  
  + So the contribution of this subinterval to the difference $U(f,P) - L(f,P)$ is $(f(t_k) - f(t_{k-1}))  (t_k - t_{k-1})$.
  By construction of $P,$ this is less than $$(f(t_k) - f(t_{k-1}) \frac{\epsilon}{f(b)-f(a)}.$$
  
  + Now sum over all the subintervals to find that
  $$U(f,P) - L(f,P) < \frac{\epsilon}{f(b)-f(a)} \sum_{k = 1}^n (f(t_k) - f(t_{k-1}) = \frac{\epsilon}{f(b)-f(a)} (f(b)-f(a))$$
  
  + It follows that for partition $P$, $U(f,P) - L(f,P) < \epsilon$, which guarantees integrability.
  
  * A similar argument shows that any decreasing function is integrable, and that any "piecewise monotone" function, one that switches from increasing to decreasing only finitely many times on $[a,b]$, is integrable.
  
# Any continuous function on $[a,b]$ is integrable
 * Recall that if function $f$ is continuous on a closed interval, it is also uniformly continuous on that interval. Uniform continuity means that we can choose $\delta >0$ such that if $x,y \in [a,b]$ and $|x-y| < \delta$, then $|f(x) - f(y)| < \frac{\epsilon}{b-a}$
 
 * So, given $\epsilon > 0$, we just choose a partition $P$ with mesh $< \delta.$ On any subinterval $[t_{k-1},t_k]$ the contribution to the difference $U(f,P) - L(f,P)$ is $(M(f,[t_{k-1},t_k]) - m(f,[t_{k-1},t_k]) (t_k - t_{k-1})$, which is less than 
 $\frac{\epsilon}{b-a} (t_k - t_{k-1})$
 
 + Now sum over all the subintervals to find that
  $$U(f,P) - L(f,P) < \frac{\epsilon}{b-a} \sum_{k = 1}^n (t_k - t_{k-1}) = \frac{\epsilon}{b-a} (b-a).$$
  
 + It follows that for partition $P$, $U(f,P) - L(f,P) < \epsilon$, which guarantees integrability.
 
 * A similar argument shows that any "piecewise continuous" function, one that has only finitely many points of discontinuity on $[a,b]$, is integrable.
  
# Sum of two integrable functions
  
  * For two functions $f$ and $g$, both integrable on the interval $[a, b]$, their sum is also integrable and is equal to the sum of their integrals:
  $$\int_a^b f +g  = \int_a^b f + \int_a^b g$$
  
* Proof: Consider a subinterval $S$ in partition $P$.
  
  + $\forall x \in S, f(x) \geq m(f, S)$, and $\forall x \in S, g(x) \geq m(g, S)$  
  
  + $\forall x \in S, (f+g)(x) \geq m(f,S) +m(g, S)$ and therefore $m(f+g, S) \geq m(f,S) +m(g, S)$
  
  + On summing over all the subintervals in partition $P$, we conclude that
  $L(f+g, P) \geq L(f,P) +L(g, P)$ 
  
  + A similar argument shows that $U(f+g, P) \leq U(f,P) +U(g, P)$
  
  + It follows that $L(f,P) +L(g, P) \leq L(f+g, P) \leq U(f+g, P)   \leq U(f,P) +U(g, P)$. 
  
  + Since $f$ is integrable, we can choose $P_1$ so that $U(f,P_1) - L(f,P_1) < \frac{\epsilon}{2}$ and we can choose $P_2$ so that $U(g,P_2) - L(g,P_2) < \frac{\epsilon}{2}$
  
* Now construct the finer partition $P = P_1 \cup P_2$ so that 
   $U(f,P) - L(f,P) < \frac{\epsilon}{2}$ and $U(g,P) - L(g,P) < \frac{\epsilon}{2}$  
  It follows that $U(f+g,P) - L(f+g,P) < \epsilon$, which shows that $f+g$ is integrable.
  
* Since $L(f+g, P)$ is squeezed between $L(f,P) +L(g, P)$ and $U(f,P) +U(g, P)$, whose difference can be made less than any $\epsilon >0$, we see that $L(f+g,P) = L(f,P) +L(g,P)$ and so $\int(f+g) = \int f+ \int g.$

  
* This result can easily be extended to the sum of any finite sum of integrable functions. It does not extend to infinite sums of integrable functions.
  
  
# A useful corollary
 * If $f(x) \leq g(x)$ on $[a,b]$, then $$ \int_a^b f \leq \int_a^b g$$
 
 * Proof: set $g(x) = f(x) +  (g(x)-f(x))$
 
   + Then, by the preceding result, $$\int_a^b g = \int_a^b f + \int_a^b (g-f).$$
   
   + But $g(x) - f(x) \geq 0$, so $g-f$ is nonnegative and $\int_a^b (g-f) > 0$
   
   + It follows that $$\int_a^b g \geq \int_a^b f $$
  
  
# Evaluating an integral by Darboux sums.
  * It is easily proved by induction that $$\sum_1^n k^2 = \frac{n(n+1)(2n+1)}{6}.$$
  
  * We want to calculate $\int_0^b x^2 dx$. Since the function $f(x) = x^2$ is both monotone and continuous on $[0,b]$, we know that it is integrable, and it will suffice to calculate $L(f).$ This is easy, since the minimum in every subinterval is at the left endpoint.
  
  * As a partition $P$, break $[0,b]$ into $n$ subintervals of equal length. Then  
  $$L(f,P) = \sum_{k=0}^n (\frac{kb}{n})^2 \frac{b}{n} = (\sum_{k=0}^n k^2) \frac{b^3}{n^3} $$
  
  * Thanks to the summation formula above, we have
  $$L(f,P) = \frac{(n-1)n(2n+1)}{6 \cdot n \cdot n \cdot n} b^3$$
  
  * Finally, take the limit as $n \rightarrow \infty$
  $$L(f) = \lim (1 - \frac{1}{n})(2 - \frac{1}{n})\frac{b^3}{6} = \frac{b^3}{3}$$
  
  

  * Before Newton and Leibniz invented differential calculus, great mathematicians like Wallis (1616-1703) and Jyesthadeva (1500-75) had to use this approach to calculate the integrals they needed.
  
# A function that is integrable in spite of being discontinuous at all rational $x$

 * For irrational $x$, $f(x) = 0$; for rational $x = \frac{p}{q} \text{ (in lowest terms)}$, $f(x) = \frac{1}{q}$.
 
 * The challenge is to show that $\int_0^1 f = 0.$ 
 
 * Clearly, for any partition $P$,  $L(f,P) = 0$. We need to show that for any $\epsilon$, we can construct a partition $P$ for which $U(f,P) < \epsilon.$
  
 * First choose an integer $q$ so large that $q \epsilon > 2$. Then $\frac{1}{q} < \frac{\epsilon}{2}.$  
 There are only $N_q$ fractions with denominator less than $q$. Choose any partition $P$ in which each is trapped in a subinterval of length $\frac{\epsilon}{2N_q}$ 
 
 + Since each fraction is less than 1, these subintervals contribute less than
 $$N_q \cdot \frac{\epsilon}{2N_q} = \frac{\epsilon}{2} $$ to $U(f,P).$
 
 + The remaining intervals have combined length less than 1 and the maximum value is less than $\frac{\epsilon}{2}$. So they contribute less than $\frac{\epsilon}{2}$ to $U(f,P).$
 
 + It follows that $U(f,P) -L(f,P) < \epsilon$, Therefore $f$ is integrable, and the value of the integral is $L(f,P) = 0$.
 
# What about Riemann integrals?
    
   * A **Riemann sum** of $f$ associated with a partition $P$ is a sum of the form $S = \sum_{k=1}^n f(x_k)(t_k-t_{k-1})$
    where $x_k \in [t_{k-1}, t_k]$ for $k = 1, 2, 3 \cdots, n$.    There are many possible rules(e.g. left, right, midpoint) for choosing $x_k$; so there are many Riemann sums for a given function and partition.
  
```{r, echo = FALSE, asp = 1, fig.width= 4, fig.height = 4}
  curve(1*x, from =0, to = 1,  xlab = "", ylab = "", xaxt = "n" ,yaxt = "n") 
  points(c(0,.5, 1),c(0,.5,1))
  text(0,0, "0"); text(0.5,0.1, "h/2"); text(1,0, "h")
  text(0,0, "0"); text(0.1,0.5, "h/2"); text(0,1, "h")
``` 

  * For the function $f(x) = x$, with a single subinterval, the "midpoint" Riemann sum and the "trapezoid" Riemann sum both give the exact value of the integral $\int_0^h x dx.$
  * Midpoint: $f(\frac{h}{2})\cdot h = \frac{h^2}{2}$
  * Trapezoid: $\frac{1}{2}(f(0)+f(h))\cdot h = \frac{h^2}{2}$
  * Exact: $\int_0^h x dx = \frac{h^2}{2}$
  
# Riemann sums for $f(x) = x^2$
  
  * For the function $f(x) = x^2$, with  a single subinterval, a mixture of 2/3 "midpoint" Riemann sum and 1/3 "trapezoid" Riemann sum gives the exact value of the integral  $\int_0^h x^2 dx.$
  
  * Midpoint: $f(\frac{h}{2})\cdot h = \frac{h^2}{4}\cdot h = \frac{h^3}{4}$
  
  * Trapezoid: $\frac{1}{2}(f(0)+f(h))\cdot h =  \frac{h^3}{2}$
  
  * Mixture: $\frac{2}{3} \cdot \frac{h^3}{4} + \frac{1}{3} \cdot \frac{h^3}{2} = \frac{h^3}{6} + \frac{h^3}{6} = \frac{h^3}{3}.$
  
  * Exact: $\int_0^h x^2 dx = \frac{h^2}{3})$
  
```{r, echo = FALSE, asp = 1, fig.width= 4, fig.height = 4}
  curve(x^2, from =0, to = 1, , xlab = "", ylab = "", xaxt = "n" ,yaxt = "n") 
  points(c(0,.5, 1),c(0,.25,1))
  text(0,0, "0"); text(0.5,0.1, "h/2"); text(1,0, "h")
  text(0.05,0.1, "0"); text(0.05,0.25, expression(h^2/4)); text(0.05,1, expression(h^2))
``` 

# Simpson's rule
  
* Since each interior division point contributes to two trapezoid sums, we assign a weight of 1/6 to the two endpoints,  a weight of 1/3 to each interior division point, and a weight of 2/3 to each midpoint. The result, called "Simpson's rule," turns out to be exact for all polynomial functions of degree 3 or less and quite good for functions that are well approximated by polynomials.

* For example, we can use it to approximate  $\log_e{2} =\int_2^4 \frac{1}{x}dx = 0.693147$, using just a single interval. 

```{r, echo = TRUE, fig.width = 6, fig.height = 4}
  curve(1/x, from = 1, to = 4); abline(v = c(2,3,4)) ; abline(h = c(1/2,1/3,1/4))
    ((1/6)*(1/2)+ (2/3)*(1/3)+ (1/6)*(1/4))* 2
```




# Riemann integrability

* We say a function is **Riemann integrable**, with Riemann integral $r$, if there exists a number $r$ such that $\forall \epsilon >0$, $\exists \delta >0$ such that, for every Riemann sum $S$ and for every partition $P$ whose mesh (longest subinterval) is less than $\delta,$ $|S-r| < \epsilon.$


* If a bounded function is Darboux integrable, it is Riemann integrable, and the values of the integrals agree. The converse is also true.  
Proof: for any partition $P:$

 
 + Lower Darboux sum: $L(f,P) = \sum_{k=1}^n m(f,[t_{k-1},t_{k}))(t_k - t_{k-1})$
 
 + Riemann sum: $S = \sum_{k=1}^n f(x_k)(t_k - t_{k-1})$ where $t_k \leq x_k \leq t_{k-1}$
 
 + Upper Darboux sum: $U(f,P) = \sum_{k=1}^n M(f,[t_{k-1},t_{k}))(t_k - t_{k-1})$
 
* No matter what rule if used to select the evaluation point $x_k$,
 $m(f,[t_{k-1},t_{k})\leq f(x_k) \leq M(f,[t_{k-1},t_{k})$, and so $L(f,P) \leq S \leq U(f,P).$
 
* If $f$ is Darboux integrable, the Riemann integral is "squeezed" between $L(f,P)$ and $U(f,P)$, and so $L(f) = S = U(f)$.

* You can think of the lower Darboux integral as a "lower Riemann integral," with $x_k$ chosen to give the lowest value in each subinterval. Similarly, the upper Darboux integral is an "upper Riemann integral." If $f$ is Riemann integrable, any selection criterion leads to the same value for the integral in the limit as the mesh of $P$ goes to zero. Therefore the lower and upper Darboux integrals are equal, and $f$ is also Darboux integrable.

# Monte Carlo integration

 * Another rule for choosing the evaluation point for a Riemann integral is "choose a random evaluation point in each subinterval." 
 
 * An even more extreme form of "Monte Carlo integration" is "choose $n$ evaluation points at random in the interval $[a,b].$"   
This approach, though not very fruitful in one dimension, is sometimes the only feasible approach for evaluating messy integrals in $\mathbb{R}^n$.

* Here is a comparison of different approaches.
```{r, echo = TRUE}
#Choose a function to integrate.
  f <- function(x) tan(x)
#Choose endpoints
  a <- pi/4; b <- pi/3
#Choose the number of equal subintervals
  n <- 10
#Construct the partition with n equal subintervals
  Part <- seq( from = a, to = b, length.out = n+1); Part #first index is 1
  RsumLeft <- RsumRight <- RsumMidpoint <- RsumRandom1 <- RsumRandom2 <- RsumMonteCarlo <- 0      #to tally the Riemann sums
  for (i in 1:n) {
    RsumLeft <- RsumLeft + f(Part[i])
    RsumRight <- RsumRight + f(Part[i+1])
    RsumMidpoint <- RsumMidpoint + f((Part[i]+Part[i+1])/2)
    xEval <- runif(1,min=Part[i],max=Part[i+1])  #choose a random point
    RsumRandom1 <- RsumRandom1 + f(xEval)   #evaluate at a random point
    xEval <- runif(1,min=Part[i],max=Part[i+1])  #choose a different random point
    RsumRandom2 <- RsumRandom2 + f(xEval)   #evaluate at a random point
    xEval <- runif(1,min=a,max=b)  #choose a random point anywhere in [a,b]
    RsumMonteCarlo <- RsumMonteCarlo + f(xEval)
  }
#Ask R for the exact answer so we can compare
  -log(cos(b)) + log(cos(a))     #for the tangent function we know an antiderivative
  integrate(tan, lower = a, upper = b)  #the R built-in function does very well
  RsumLeft * (b-a)/n     #too low, since the tangent function is increasing
  RsumRight * (b-a)/n    #too high, since the tangent function is increasing
  RsumMidpoint * (b-a)/n #much better estimate
  (RsumLeft/6+ RsumRight/6+2*RsumMidpoint/3)* (b-a)/n   #Simpson gets 7 decimal places correct
  RsumRandom1 * (b-a)/n   #better than left or right
  RsumRandom2 * (b-a)/n   #different random choices, different answer
  RsumMonteCarlo * (b-a)/n   #random choices may not be well spaced; could be way off
```

# The dyadic approach to Riemann integrals
* This is the approach used by Hubbard. It combines features of the Darboux approach and the Riemann approach and generalizes nicely to $n$ dimensions.

* Define the **support** of a function $f$ to be the set on which $f(x) \neq 0$. A function has  **bounded support**  if this set is contained within some bounded interval $[a,b]$.
We can always choose $a$ and $b$ to be integers.

* Here is the dyadic way to compute $\int_{-\infty}^{\infty}f(x)dx$ for a
bounded function with bounded support.

+ Choose an interval $[a,b]$ with integer $a$ and $b$ that includes the support of the function.

+ Break it into $(b-a)2^N$ equal pieces, each of length $\frac{1}{2^N}$. 

+ Evaluate the upper Riemann sum $U_N(f) = \sum_CM_C(f)\frac{1}{2^N}$, summing over the pieces.

+ Evaluate the lower Riemann sum $L_N(f) = \sum_Cm_C(f)\frac{1}{2^N}$, summing over the pieces.

+ If $U_N(f)$ and $L_N(f)$ have the same limit as $N \rightarrow
\infty$, this common limit is the Riemann integral.

* If $P_N$ is the partition at level $N$ and $P_{N+1}$ is the partition at level $N+1$, then every subdivision point in $P_N$ was already in $P_{N+1}$; so $P_N \subset P_{N+1}$ .

* Therefore $L_N(f)$ is an increasing sequence and $U_N(f)$ is a decreasing sequence. The lower Darboux integral can now be defined as $\lim L_N(f)$, the upper Darboux integral can now be defined as $\lim U_N(f)$, and the condition for integrability is simply that $\lim (U_N(f) - L_N(f)) = 0$. There is no longer any mention of arbitrary partitions!

# Integration of indicator functions

* As before, we define the **indicator function** $\mathbf{1}_A(x)$ of set $A \subset \mathbb{R}$ to have the value 1 if $x \in A$, 0 if $x \notin A$.

* If the indicator function $\mathbf{1}_A$ of set $A$ is integrable,
 then we define the $1$-dimensional volume of $A$ as the integral of $\mathbf{1}_A$
 over $\mathbb{R}$. Thus the interval [0,1] on the $x$-axis has a
 1-dimensional volume (which you can also refer to as "length") of 1.

* It seems obvious (Hubbard Lemma 4.1.19) that the interval $I=[a,b]$ has
 1-dimensional volume $|b-a|$, but this is a result that requires proof. The complication is if the endpoints $a$ and $b$ are irrational, they will always lie in the interior of a dyadic subinterval.
 
# Defining length by integration

* This approach seems bizarre in one dimension, but it will provide a way to define "volume" in more than three dimensions, where our intuition fails.

* Let $a$ and $b$ be irrational numbers with $a < b$. We can use the dyadic definition of the Riemann integral to prove that 

$$\int_a^b |dx| = b-a$$
```{r, echo = FALSE, , fig.width= 8, fig.height = 1.8}
 par(mar = c(1,1,1,1))
 plot(c(0.22,0.25, 0.75,0.78), c(0,0,0,0), xlim = c(0,1), ylim = c(-0.05,0.1))

 text(c(0.22,0.25, 0.75,0.78), c(-0.02,-0.02,-0.02,-0.02),c("a",expression("a"[N]),expression("b"[N]),"b"))
 segments(c(0.20,0.25,0.75, 0.8),c(-0.05,-0.05,-0.05,-0.05),c(0.20,0.25,0.75, 0.8),c(0.05,0.05,0.05,0.05), col = "red")
 segments(c(0, 0.22,0.78),c(0,0.04,0),c(0.22,0.78,1),c(0,0.04,0))
```

+ Let $a_N$ be the right endpoint of the dyadic subinterval that includes $a$.

+ Let $b_N$ be the left endpoint of the dyadic subinterval that includes $b$.

+ On these two subintervals, $M(\mathbf{1}_{a,b}) = 1, m(\mathbf{1}_{a,b}) = 0)$

+ Each has length $\frac{1}{2^N}$ ; so together they contribute $\frac{2}{2^N}$ to the difference $U_N - L_N.$

+ Therefore $\lim(U_N - L_N) = 0$ and $\mathbf{1}_{a,b}$ is integrable.

+ Since $b_N$ keeps getting closer to $b$ as the dyadic subintervals get smaller, $b_N$ is an increasing sequence that converges to $b$. 

+ Since $a_N$ keeps getting closer to $a$ as the dyadic subintervals get smaller, $a_N$ is an decreasing sequence that converges to $a$.

+ At any level $N$ of dyadic subdivision, $L_N = b_N - a_N$, since only the subintervals between $a_N$ and $B_N$ have $m(\mathbf{1}_{a,b})= 1$ 

* So $\lim L_N = \lim(b_N - a_N) = \lim b_N - \lim a_N = b-a$


# Fundamental Theorem of Calculus Version I -- integral of the derivative

* Suppose that function $g$ is continuous on $[a,b]$ and differentiable on $(a,b)$, and that $g'$ is integrable on $[a,b]$. Then
  $$\int_a^bg' = g(b) - g(a).$$

* To prove the theorem, exploit the fact that $g'$ is integrable to choose a partition $P$ such that  
$U(g',p) - L(g',P)<\epsilon$

* For any subinterval in the partition, $g$ is continuous on the closed interval $[t_{k-1},t_k]$ and differentiable on the open interval $(t_{k-1},t_k)$. It satisfies the conditions of the mean value theorem, and we can select $x_k \in (t_{k-1},t_k)$ for which $$g'(x_k) = \frac{g(t_k) - g(t_{k-1})}{t_k - t_{k-1}}$$

* Evaluate the integral of $g'$ as a Riemann integral, using $x_k$ as the evaluation point in each subinterval. Then
$$S = \sum_{k=1}^n g'(x_k)(t_k - t_{k-1}) = \sum_{k=1}^n (g(t_k) - g(t_{k-1})) =g(b) - g(a)$$



# Integration by parts

* This is a corollary of Version I of the Fundamental Theorem.

* Set $g = uv$. Then $g' = uv' + u'v$.

* By the Fundamental Theorem,
$$\int_a^b g'(x) dx = \int_a^b u(x)v'(x) dx + \int_a^b u'(x)v(x) dx = g(b) - g(a)$$

* It follows that 
$$\int_a^b u(x)v'(x) dx = u(b)v(b) - u(a)v(a) - \int_a^b u'(x)v(x) dx$$

* A somewhat careless shorthand version is
$$\int_a^b u dv = uv]_a^b - \int_a^b v du$$

# Integration by parts to get a recursion

* Before calculus was invented, English mathematician/theologian John Wallis found a way to approximate $\pi$ by interpolation. He needed to evaluate integrals of the form

$$A_n = \int_{-1}^1 (1-x^2)^n dx.$$

* When $n = \frac{1}{2}, A_{1/2}= \frac{\pi}{2}$, the area of the unit semicircle.

* Wallis, by looking at small integer values of $n$, correctly guessed that for $n > 0,$ $$A_n = \frac{2n}{2n+1}A_{n-1}.$$

# Proof of Wallis's recursion

 * We can prove this formula using integration by parts. Split off one factor of $1-x^2$ to get

 $$A_n = \int_{-1}^1 (1-x^2)^{n-1} (1-x^2) dx = \int_{-1}^1 (1-x^2)^{n-1}  dx- \int_{-1}^1 (1-x^2)^{n-1} x^2 dx $$
 Integrate the second term by parts,   
choosing $u(x) = x, v(x) = \frac{-1}{2 \pi}(1-x^2)^n$
so that $v'(x) = x(1-x^2)^{n-1}$. Then $$\int_{-1}^{1}x\cdot x(1-x^2)^{n-1}= - \frac{x}{2n}(1-x^2)]_{-1}^{1} + \int_{-1}^{1} 1 \cdot\frac{1}{2n}(1-x^2)^n.$$

 * Conclusion: $$A_n = A_{n-1} - \frac{1}{2n} A_n,$$
$$2nA_n = 2nA_{n-1} - A_n,$$
$$A_n = \frac{2n}{2n+1} A_{n-1}$$

# A formula for Wallis's integral

* We have shown that $$A_n = \frac{2n}{2n+1} A_{n-1}.$$ 

* Now we can prove by induction that for $n \geq 0$, $$A_n = \frac{2^{2n+1}(n!)^2}{(2n+1)!}.$$

* Base case: $n = 0,  A_0 = \frac{2 \cdot 1^2}{1} = 2$ and $\int_{-1}^1 1 dx = 2$.

* Inductive step: use the given formula for $A_{n-1}$

$$A_n = \frac{2n}{2n+1} A_{n-1} = \frac{2n}{2n+1} \frac{2^{2n-1}(n-1)!^2}{(2n-1)!}\cdot\frac{2n}{2n} = \frac{2^{2n+1}(n!)^2}{(2n+1)!}$$

# Fundamental Theorem of Calculus Version II - derivative of the integral

* Suppose that $f$ is an integrable function on $[a,b].$   
For $x \in [a,b]$, define the antiderivative
$$F(x) = \int_a^xf(t)dt.$$  
Then $F$ is uniformly continuous on $[a,b].$ 

* Proof: $f$, being continuous on a closed interval, is **bounded**, $|F(x)| < B, \forall x \in [a,b]$

+ Given $\epsilon > 0$, choose $x < y \in [a,b]$ with $y-x < \frac{\epsilon}{B}$.

+ Then $|F(y) - F(x)| = \int_a^y f(t)dt - \int_a^x f(t) dt = \int_x^y f(t) dt < (y-x)B$

+ So $|F(y) - F(x)| < \frac{\epsilon}{B} \cdot B < \epsilon$  and $F$ is uniformly continuous.

# Differentiability of $F$ 

* At any point $x_0 \in [a,b]$ where $f$ is continuous, $F$ is differentiable and $F'(x_0) = f(x_0).$

* Proof for the case where $x > x_0$
$$\frac{F(x)- F(x_0)}{x-x_0}- f(x_0) = \frac{1}{x-x_0}\int_{x_0}^xf(t)dt - \frac{1}{x-x_0}\int_{x_0}^xf(x_0)dt = \frac{1}{x-x_0}\int_{x_0}^x(f(t)-f(x_0))dt.$$

* When $f$ is continuous at $x_0$, we can choose $\delta < 0$ such that if $t-x_0 < \delta$, then $|f(t) - f(x_0)| < \epsilon.$

* Then $$\frac{F(x)- F(x_0)}{x-x_0}- f(x_0) < \frac{1}{x-x_0}\cdot \epsilon \cdot (x - x_0).$$

* This shows that $F'(x_0) = f(x_0)$
   
# Mass functions and summation

* When the sample space is $\mathbb{R}$, a **discrete distribution** is completely described by its **mass function,** which specifies the probability for each of a countable number of values in the sample space. The probability of any event can be calculated by summing over the values that make up the event. Recall how this works in R for a binomial distribution.
```{r, echo = TRUE}
  n <- 50 #trials
  p <- 0.3 #probability of success
  barplot(dbinom(0:30, n, p),names.arg = 0:30)   #tells the whole story
  dbinom(18, n, p)    #probability of a single value
  pbinom(18,n,p); sum(dbinom(0:18,n,p))  #probability for X less than or equal to x 
  sum(dbinom(seq(from = 10, to =20, by = 2),n,p)) #probability of an even value between 10 and 20
  sum(dbinom(0:n,n,p))  #total probability must equal 1
```

# Density functions and integration

* When the sample space is $\mathbb{R}$, a **continuous distribution** can be completely described by its **density function,** which specifies the probability for any subinterval of the sample space. The probability of the event $X \in [a,b]$ can be calculated by integrating the density function over the interval. Here is an illustration of how this works in R for a continuous exponential distribution, which closely resembles a discrete geometric distribution.
```{r, echo = TRUE}
lambda <- 0.2  #the one parameter for an exponential distribution
curve(dexp(x, lambda), from = 0, to = 30)   #the whole story
integrate(function(x) dexp(x, lambda), lower = 0, upper = 100) #total probability must be 1
integrate(function(x) dexp(x, lambda), lower = 0, upper = 10)  #probability that X is 10 or less
pexp(10, lambda)   #this function has the same meaning for discrete or continuous
samples <- rexp(10000, lambda)  #this generates random samples
head(samples)    #these are real numbers, not integers
#A histogram, when converted to probabilities, matches the graph of the density function
hist(samples,breaks = 30,prob = TRUE, add = TRUE)
```

# Using the density and distribution functions

```{r}
integrate(function(x) dexp(x, lambda), lower = 2, upper = 4) #probability that X is between 2 and 4
pexp(4, lambda) - pexp(2, lambda)  #another way
samples <- rexp(10000, lambda)  #generate random samples
mean((samples > 2) & (samples < 4))  #proportion of the samples between 2 and 4 
pexp(2.000001, lambda) - pexp(1.999999, lambda)  #probability that a sample is exactly 2 is zero
dexp(2,lambda)    #this is a probability density
dexp(2,lambda) * .02  #probability of being in an interval of length .02 near 2
pexp(2.01, lambda) - pexp(1.99, lambda)
```

# Improper integrals
  
  * The Riemann integral imposes severe requirements for a function $f(x)$ to be integrable. The interval of integration $[a,b]$ must be bounded, and the function $f$ must be bounded on this interval.
   
  * When the limit
  $$\lim_{d \rightarrow b^-} \int_a^d f(x)dx$$
    exists (perhaps finite, perhaps $\pm \infty$)  then we say the limit exists as an **improper integral.**
  
  * The limit might exist even if $b = +\infty$ (unbounded interval of integration) or if $f$ is unbounded on $[a,b)$ although bounded on $[a,d]$ for $d < b$. 
  
  * In general, theorems about integration do not necessarily apply to improper integrals. However, when the integral can be justified on the basis of the axiom of countable additivity, then we will learn that it can be interpreted as a **Lebesgue integral,** for which the usual theorems apply.
  
# An improper integral with unbounded support.

 * For an exponential distribution with parameter $\lambda = 1$, the density function is $f(x) = e^{-x}.$
 
 * To confirm that the total probability is 1, we must integrate this from 0 to $\infty$.
 
 * Here is the correct way to do this, as an improper integral
 $$\int_0^{\infty} e^{-x} dx = \lim_{a \rightarrow \infty} \int_0^{a} e^{-x} dx = \lim_{a \rightarrow \infty} \int_0^{a} e^{-x} dx= 1 - e^{-a} = 1$$
 
* In this case the improper integral is justified by the following argument:
 
  + The events of the form $k < X \leq k+1$ are a countable set of disjoint events.
 
  + The union of these events is the event $0 < X$ (no upper limit)
 
  + The probability of that event is $$\sum_{k = 0}^\infty (k < X \leq{k+1})= \lim \sum_{k = 0}^n \int_k^{k+1}e^{-x}dx = \lim  \int_0^{n}e^{-x}dx$$.
 
 + That is exactly the same as evaluating the improper integral.

# An improper integral with an unbounded function.

 * The function $f(x) = 0.5 x^{-0.5}$ is a density function on $(0, 1]$ whose only problem is that it is unbounded. 
 
 * No problem: the events $\frac{1}{2} < X \leq 1, \frac{1}{4} < X \leq \frac{1}{2},  \frac{1}{8} < X \leq \frac{1}{4}, \cdots$  
 are a countable set of disjoint events, and we are allowed to calculate and add their probabilities to get
  $$\sum_{k=1}^{\infty}\int_{2^{-(k+1)}}^{2^{-k}} 0.5 x^{-0.5} dx = \lim_{k \rightarrow \infty}\int_{2^{-(k+1)}}^1 0.5 x^{-0.5} dx.$$
 
 * This is exactly the same as calculating the improper integral
 $$\int_0^1 0.5  x^{-0.5} dx = \lim_{a \rightarrow 0} \int_a^1  x^{-0.5} dx = \lim_{a \rightarrow 0}(1 - a^{0.5}) = 1$$
 

# The most important improper integral in mathematics

 * Define $I(a) = \int_{-a}^{a}e^{-x^2} dx =  \int_{-a}^{a}e^{-y^2} dy$  There is no formula, in terms of familiar functions, for the antiderivative of the function $e^{-x^2}$, but we can still evaluate the limit of $I(a)$ as $a \rightarrow \infty$ using Riemann integrals.
```{r, echo = FALSE, warning = FALSE, asp = 1, fig.width= 3.5, fig.height = 3.5}
library(plotrix)
par(mar = c(0,0,0,0))
plot(NULL, asp = 1, xlim = c(-2.8,2.8), ylim = c(-2.8,2.8), bty = "n", xaxt= "n", yaxt = "n", xlab = "", ylab= "n")
draw.circle(0, 0,2)
draw.circle(0, 0,2*sqrt(2))
points(c(-2,-2,2,2,-2), c(-2,2,2,-2,-2), type = "l")
text(2.4,2,"(a,a)")
text(2.4,-2,"(a,-a)")
text(-2.4,2,"(-a,a)")
text(-2.4,-2,"(-a,-a)")
points(c(-2,2), c(1.5,1.5), type = 'l',lty = 2)
points(c(-2,2), c(1.4,1.4), type = 'l',lty = 2)
draw.circle(0, 0,1.2, lty = 2)
draw.circle(0, 0,1.1, lty = 2)
text(0,1.5, "strip")
text(1.1,0, "ring")
```
  
 



* Consider a metal plate whose mass per unit area is $\sigma\begin{pmatrix}x \\ y\end{pmatrix}= e^{-x^2} e^{-y^2}$. It is infinite in extent, but we can consider a square piece of side $a$, and we can also consider circular pieces inscribed in the square and circumscribed about it.
  
* View the square piece as made up of horizontal strips. A strip at $y$ with width $\Delta y$ has mass $I(a)e^{-y^2} \Delta y$.    So the total mass is $\int_{-a}^a I(a) e^{-y^2} dy = I(a)^2.$

* View the inscribed circular piece as made up of concentric rings. A ring at $r$ with width $\Delta r$ has mass $e^{-r^2} \cdot 2 \pi r \Delta r$.

* The total mass of the inscribed disk of radius $a$ is (with $u = r^2, du = 2r dr$) $$\int_0^a 2 \pi r e^{-r^2}rd = \int_0^{a^2} \pi e^{-u} du = \pi(1 - e^{-a^2}).$$
The circumscribed disk of radius $\sqrt{2}a$ has mass $\pi(1 - e^{-2a^2}).$ 
So $\pi(1 - e^{-a^2}) < I(a)^2 <\pi(1 - e^{-2a^2}).$

* As $a \rightarrow \infty$, both the left term and the right term approach a limit of $\pi.$  
  So by the squeeze lemma, $\lim_{a \rightarrow \infty}I(a)^2 = \pi$ and $\lim_{a \rightarrow \infty}I(a) = \sqrt{\pi}$.


# The gamma function -- a generalization of the factorial function
 
* Define the gamma function for $r > 0$ by $\Gamma(r) = \int_0^{\infty}x^{r-1}e^{-x}dx$ (an improper integral). 
Here are its key properties.

  * $\Gamma(1) = \int_0^{-\infty}x^0 e^{-x} dx = 1.$
  
  * Do integration by parts with $u = x^r, u' = rx^{r-1},v = -e^{-x}, v' = e^{-x}$ to find  
  $$\Gamma(r+1) = \int_0^{-\infty}x^r e^{-x} dx = -x^r e^{-x} ]_0^{\infty}+ r\int_0^{-\infty}x^{r-1} e^{-x} dx = r\Gamma(r)$ if $r >0.$
  
  * $\text {For integer } n > 0,  \Gamma(n+1) = n!$  
  Proof is by induction. Base case $n=0$, $\Gamma(1) = 0! = 1$  
  Inductive step $\Gamma(n+1) = n \Gamma(n) = n(n-1)! = n!.$
  
  
  * $\Gamma(\frac{1}{2}) = \int_0^{-\infty}x^{-\frac{1}{2}} e^{-x} dx.$  
  Set $x = u^2, dx = 2u du$. 
  to find
  $\Gamma(\frac{1}{2}) = \int_0^{\infty}u^{-1}e^{-u^2} 2u du = 2 \int_0^{\infty}e^{-u^2}du = \int_{-\infty}^{\infty}e^{-u^2}du = \sqrt{\pi}$
  
# Evaluating the gamma function
  
  * For positive integer arguments, the gamma function is just the familiar factorial function, so  
  $\Gamma(1) = 0! = 1, \Gamma(2) = 1! = 1, \Gamma(3) = 2! = 2, \Gamma(4) = 3! = 6.$
   
  * For half-integer arguments, the gamma function can be expressed in terms of $\Gamma(\frac{1}{2})$. For example,    
   $\Gamma(\frac{7}{2}) = \frac{5}{2} \Gamma(\frac{5}{2}) = \frac{5}{2} \frac{3}{2} \Gamma(\frac{3}{2}) =  \frac{5}{2} \frac{3}{2} \frac{1}{2}\Gamma(\frac{1}{2}) = \frac{15}{8} \sqrt{\pi} = 3.3233$    
  which is in between $\Gamma(3) = 2$ and $\Gamma(4) = 6.$
   
  * R provides full support for the gamma function.
```{r}
gamma(3); gamma(7/2); gamma(4)
curve(gamma, from = 0.25, to = 4.5)
abline(v = 1:4, col = "red")
abline(h = c(1,2,6), col = "blue")
```
  
  
  
  


# The normal and exponential distributions

* Suppose that $X$ is a random variable and we want to know the probability of the event $A$: $a < X \leq b$. When $X$ is a "continuous random variable," which means that the probability of the event $X = x$ is zero for all $x$, we can calculate the probability of $A$ by integrating a **probability density function** $\mu_X(x)$:
  $$P(A) = \int_a^b \mu_X(x) dx.$$
  
* The function $\mu_X(x)$ must be non-negative, and it must have the property that $\int_{-\infty}^{\infty} \mu_X(x) dx = 1.$ We can now check this property for two important cases.

  + The exponential distribution, with $\mu(x) = \lambda e^{-\lambda x}$ for $x \geq 0$.
$$\int_{-\infty}^{\infty} \lambda e^{-\lambda x} dx = \lambda \cdot \frac{1}{\lambda} = 1.$$

  + The standard normal distribution, with $$\mu(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}.$$  
$$\text{ Set }x = \sqrt{2}u, dx = \sqrt{2}u \text{ to get } \int_{-\infty}^{\infty}\frac{1}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{\infty}e^{-u^2}\sqrt{2}du = \frac{1}{\sqrt{\pi}}\sqrt{\pi} = 1$$
  
  

# The gamma distribution

* Recall that the sum of $r$ random variables, each having a geometric distribution with the same parameter $\lambda$, is a negative binomial distribution. We can check this property by simulation:
```{r, echo =TRUE}
r <- 4; lambda <- 0.1
#Add together $r$ samples from a geometric distribution, and do it 20000 times
#Add one to each sample to get the "until" version of geometric
N <- 20000; GSamples <- numeric(N)
for (i in 1:N) {
  GSamples[i] = sum(rgeom(r,lambda)+1)  #add 1 to convert to "until" version
}
hist(GSamples, breaks =60)
mean(GSamples)   #Should be close to 4*10 = 40

#Add together $r$ samples from an exponential distribution, and do it 20000 times
N <- 20000; ESamples <- numeric(N)
for (i in 1:N) {
  ESamples[i] = sum(rexp(r,lambda)) 
}
hist(ESamples, breaks =60)
mean(ESamples)
#Here is a way to plot two histograms side by side
library(plotrix)
multhist(list(GSamples,ESamples), breaks =30, prob = TRUE)

#The density function for the gamma distribution matches the histogram nicely.
hist(ESamples, breaks =60, prob = TRUE)
curve(dgamma(x, r, lambda), add = TRUE)
```

# Density function for the gamma distribution

* The gamma function is needed only to do the correct normalization.
$$\mu(x) = \frac{\lambda^r}{\Gamma(r)}x^{r-1}e^{-\lambda x}, x \geq 0$$
* Because the integral in the numerator defines the gamma function, $$\int_0^{\infty}\mu(x) dx = 1$$  
Again, the use of an improper integral is justified by the axiom of countable additivity.

* In R, the usual family of functions is available.
```{r}
#Here is the formula for the density function
mu <- function(x, r, lambda) lambda^r* x^(r-1)*exp(-lambda*x)/gamma(r)
#Thanks to the gamma function in the denominator, the total probability is 1.
#Notice that R can handle an infinite limit of integration.
integrate(mu, lambda = 0.1, r = 4, lower = 0, upper = Inf)
integrate(mu, lambda = 0.1, r = 4, lower = 0, upper = Inf)
integrate(mu, lambda = 0.1, r = 4, lower = 0, upper = 10)
pgamma(10, 4, 0.1)   #the probability that X is less than or equal to 10
#For gamma random variables with the same rate parameter lambda, the shape parameters add up.
#Add together 3 samples from an gamma distribution with r = 4, and do it 20000 times
N <- 20000; samples <- numeric(N)
for (i in 1:N) {
  samples[i] = sum(rgamma(3,shape=4, rate=0.1)) 
}
hist(samples, breaks =60, prob = TRUE)
curve(dgamma(x, shape = 12, rate = 0.1), add = TRUE)
#Any time a histogram of your data starts like a power of x and falls off exponentially,
#a gamma distribution might provide a good model.
#The shape parameter $r$ does not have to be an integer, just a real number > 0.
curve(dgamma(x, shape = 0.5, rate = 0.1))   #density can be unbounded
curve(dgamma(x, shape = 1.7, rate = 0.1), from = 0, to = 40)   #density can rise abruptly
curve(dgamma(x, shape = 8.27, rate = 1), from = 0, to = 20)   #density can rise slowly
#In every case, there is a long tail to the right.
#With rate parameter lambda = 0.5 and shape parameter df/2, the gamma distribution is the chi-square distribution
#with df degrees of freedom
curve(dchisq(x, df = 3), from = 0, to = 10)
curve(dgamma(x, shape = 3/2, rate  = 0.5), add = TRUE, col = "red") #the same density function
```


# Calculating expectation by integration

 * The expectation for a continuous random variable $X$ with density function $\mu_X(x)$ can be calculated by integrating over the sample space.
$$E(X) = \int_{-\infty}^{\infty} x\mu_X(x) dx.$$  
This formula is used as a definition in Hubbard, but it is in fact another example of the "law of the unconscious statistician."

  * When the upper limit of integration is infinity or the density function is not bounded, the integral is not a Riemann integral, but as long as the random variable $X$ is positive, we can justify the use of improper integrals by the same arguments that we used for infinite sums.
  
  * Here are a few examples in R
```{r,  echo = TRUE}
lambda = 0.2
#Expectation for an exponential distribution
#We can supply the integrand as an unnamed function of x.
integrate(function(x) x*dexp(x, rate = lambda), lower = 0, upper = Inf)
#For a gamma distribution with r = 4 the expectation should be 4 times larger
integrate(function(x) x*dgamma(x, shape = 4, rate = lambda), lower = 0, upper = Inf)
#We can also calculate variances, say for a uniform distribution in [1,7]
EXsq <- integrate(function(x) x^2*dunif(x, min = 1, max = 7), lower = 1, upper = 7); EXsq
EX <- integrate(function(x) x*dunif(x, min = 1, max = 7), lower = 1, upper = 7); EX
#The numerical value of an integral is the first item on a list of details
EXsq$value-(EX$value)^2   #the variance - theoretical
#We can check the answer by simulation
sample <- runif(10000, min = 1, max = 7)
var(sample)   #this is the correct number to compare with the population variance.
```

# An improper integral that relies on sign cancellation

* This "Fresnel integral" is important in the theory of wave optics.
$$F = \int_0^{\infty}\sin{u^2} du$$
```{r, echo = FALSE, fig.width = 10,fig.height = 4}
 curve(sin(x^2), from = 0,to = sqrt(8*pi), xlab = "u", ylab = expression(paste("sin(",u^2,")")))
abline(h = 0)
abline(v = 0)
text(1.1, 0.5, expression(A[1]))
text(2.2, -0.5, expression(A[2]))
text(2.8, 0.5, expression(A[3]))
text(3.3, -0.5, expression(A[4]))

```
  
  
* In this case the value of the integral up to any point where the integrand is zero is the sum of a finite alternating series: $I - A_1 - A_2 + A_3 - A_4 + \cdots$

* As $x \rightarrow \infty$ the infinite sum converges by the alternating series test, and its value can be confirmed by physical measurements. However, this sort of improper integral, which relies on sign cancellation, has no place in statistics or data analysis. All our infinite series and improper integrals must be **absolutely** convergent.
  

# Testing the hypothesis of a continuous distribution

  * The following numbers, taken from an online statistics book, claim to be failure times for a certain machine part. If the probability of failure is independent of how long the part has been in existence, an exponential distribution might be a good model.
  
```{r}
par(mar = c(2,2,1,1))
failTimes <- c(620,470,260,89, 388,242,103,100,39, 460,284,1285,218,393,106,158,152,477,
403,103,69, 158,818,947,399,1274, 32, 12, 134,660,548,381,203,871,193,531,
317,85, 1410, 250,41, 1101, 32, 421,32, 343,376,1512, 1792, 47, 95, 76, 515,72,
1585, 253,6,860,89, 1055, 537,101,385,176,11, 565, 164,16, 1267, 352,160,195,
1279, 356,751,500,803,560, 151,24, 689,1119, 1733, 2194, 763,555,14, 45, 776,1)
#We can estimate the parameter from the sample mean
mu <- mean(failTimes); mu
lambda <- 1/mu
hist(failTimes, breaks = 20, prob = TRUE)
curve(dexp(x, rate = lambda),col = "red", add = TRUE)
```

# Using quantiles to do a comparison

  * With a continuous distribution, the distribution function is invertible and the quantile function works well. Let's break the probability into 10 equal bins. Then, if the exponential distribution is a good model, there will be an equal number of data points in each bin.
```{r}
bins <- qexp(0.1*(0:10), lambda); bins
#The order() function returns a vector of indices for the sorted data
failSort <- failTimes[order(failTimes)]; head(failSort)
# The cut() function does exactly what we want!
cut(failSort, bins, labels = FALSE)  #default labels are integers 1:10
# In fact, the sorting was unnecessary
bincode <- cut(failTimes, bins, labels = FALSE)
head(failTimes); head(bincode) #looks reasonable
Observed <- table(bincode); Observed
Expected <- sum(Observed)/10
#Now we do the usual chi-square test
chisq <- sum((Observed - Expected)^2/Expected); chisq
#Since we estimated a parameter from the data there are only 8 degrees of freedom
pValue <- pchisq(chisq, df = 8, lower.tail = FALSE); pValue
#So the observations are consistent with an exponential distribution
#The built-in test does not know to lower the degrees of freedom.
chisq.test(Observed)
```







